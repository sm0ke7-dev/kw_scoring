Details
Iterative Script Development and Auditing Gordon Ligon emphasized the need to iteratively build the script by calculating one score or set of scores at a time and holding them in memory for the next operation. They stressed that results should be written to the sheet as a log or audit for later review, but the script should not reference the sheet for subsequent calculations, as running from the top should regenerate all scores. The location scores, for instance, should be calculated and written as a step, then kept in memory for the next operation.
Data Organization and Reference Sheet Requirements After Daniel Capistrano shared a Tampa sheet, Gordon Ligon created a new sheet named "score calculator" to organize the process. Gordon Ligon noted that the ranking data lacked the combination of "service geo keyword keyword geo" and was only showing the keyword, which was insufficient for targeting the correct row during write operations. They set up sheets for 'rankings,' 'location,' and 'keyword' data, suggesting that core location data should also be included in calculations and searches.
Location Score Calculation Methodology Gordon Ligon outlined the process for calculating the location score, which involves a normalization of market value. This calculation requires adjusting income by subtracting a poverty line value (approximately $32,000) to determine discretionary income, multiplying this by the population, and then summing all these numerical scores to find the total value of one. Each individual location's numerical score is then divided by the total numerical score to obtain a normalized location score between zero and one.
Keyword Classification and Valuation Gordon Ligon explained the importance of distinguishing between 'pure' keywords and 'location modified' keywords for scoring, determining this by checking for an exact match when controlling for capitalization. They stated their opinion that raw keywords hold more value due to general search volume. This distinction is crucial because 'keyword plus geos' multiply, potentially polluting the data if not treated differently from pure keywords.
Normalization Concept Gordon Ligon introduced the concept of normalization, which involves creating a standard scale (typically between zero and one) for a range of numbers to ensure standard and consistent calculations, especially when factoring things by multiplication. They illustrated normalization by calculating a normalized score for a day of the year (303/365), where one represents the highest possible value.
Configuring Keyword Value through Parameters A weighted method will be used to discriminate between keyword types, treating pure keywords and location modified keywords differently. This system will include two configurable parameters fed from the config: a 'partition split' to divide the logarithmic scale into two pieces for pure keywords and keyword plus geos, and a 'kappa' value to determine the slope of the decay curve. The partition split allows users to set what percentage of value is assigned to core keywords versus geo-modified keywords.
Service-Specific and Keyword Score Calculations Gordon Ligon detailed the process for calculating normalized scores for services based on an analysis similar to the Google Ads Keyword Planner, emphasizing the need to re-normalize scores if specific services like RAT are excluded to prevent skewing calculations. The service score is then multiplied by the location score to determine the total potential score for a service in a location. Keyword scores are intended to be kept "brain dead" by being calculated in layers, separated from the location and niche scores, which will be factored into a final rank calculation.
Final Rank Scoring Structure The final rank calculation will incorporate three components: a 'keyword score', a 'rank modifier score', and a 'final score', which will zip everything up. The rank modifier score will use a kappa-like function to significantly weigh high ranks (e.g., ranks 1 and 2), with any rank above 10 effectively counting as zero. Gordon Ligon noted that St. Petersburg, a high-value location, showed surprisingly good rankings, indicating that recent surge efforts were effective and providing a good target list for future surges.
Keyword Score Repetition and Scope Gordon Ligon clarified that the keyword score, when calculated without the location or niche factors, will repeat for the same search term across different locations (e.g., "wildlife pest control" will have the same keyword score in Riverview and Newport Richie). This is because the initial keyword scoring normalizes the whole set of raccoon-related keywords to be worth one, regardless of the location or niche pieces.
Calculation of Keyword Scores and Rank Score Gordon Ligon detailed the methodology for calculating various scores, starting with keyword scores which are expected to be very similar across runs because they use the exact same calculation, making auditing easier with discrete scores. They explained that the rank score will use a simple curve analysis with a kappa input, scaling values from 1 to 10, with anything above 10 being zero. Gordon Ligon noted that the final score will be a product of all other scores (location, niche, keyword, and ranking scores), which they consider the "gold mine" because it provides the actual value of rankings versus assumed values.
Competitive Analysis and Market Value Gordon Ligon highlighted that this unique product allows for market share analysis by organic search penetration for competitors simultaneously, enabling users to identify key competitors by location. They emphasized the "killer piece" of the calculation involves money times population factored against rank assumption to value services and assess targeting accuracy. Gordon Ligon also suggested segmenting keyword rank tracking by importance, using cheaper methods for less critical keywords and more rigorous tracking for important ones.
Scripting Tasks for Score Calculation Gordon Ligon outlined the distinct scripts Daniel Capistrano needs to create: location score, niche score, keyword score, ranking score, and a final merge/factor score script. For the location score, the formula involves calculating income minus poverty times population, summing those, and then normalizing the result, which they defined as calculating a normalized score across the set. The service (niche) score calculation is expected to be straightforward, involving plugging in existing data and normalizing the scores so that the normalized sum of relevant services equals one.
Keyword Score Calculation and Structure Gordon Ligon described the complexity of the keyword score script, which involves first dividing keywords by location, then by service, treating service and geo as distinct buckets for score calculation. The script also needs to categorize keywords into "core" and "geo/location" keywords and ensure that the keyword scores for similar terms, like "Riverview Raccoon" and "Port Newport Richie raccoon," are the same if the script is functioning correctly. Gordon Ligon also mentioned that the ranking score calculation should be simplified from a prior example, using a decay curve with a ranking kappa input from the config sheet.
Final Score Calculation and Data Management The final score is calculated by multiplying the location, niche, keyword, and ranking scores together. Gordon Ligon noted that if perfect rankings were achieved across all terms and markets, the sum of all final scores should ideally equal one. Daniel Capistrano was encouraged to review the recording and was given instructions to load all necessary data (location, niche, keyword rankings) at the beginning of the script and avoid referencing the written log data later to maintain an audit trail and simplify the process.

